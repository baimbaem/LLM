{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Training Overview\n",
        "\n",
        "* Session length: Halfday (theory included)\n",
        "\n",
        "* Skill level: Intermediate Python (familiar with transformers)\n",
        "\n",
        "* Goal: Students will implement an Agentic RAG loop ‚Äî an LLM that decides when to search, retrieves knowledge, and answers."
      ],
      "metadata": {
        "id": "VkMC6QZktDo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Learning Outcomes\n",
        "\n",
        "By the end of the lab, students can:\n",
        "\n",
        "\n",
        "\n",
        "*   Explain RAG and its limitations.\n",
        "*   Understand what makes an AI system agentic.\n",
        "*   Implement an agentic RAG using a small open-source model and free resources in Colab.\n",
        "*   Visualize the reasoning steps (plan ‚Üí retrieve ‚Üí generate ‚Üí reflect)."
      ],
      "metadata": {
        "id": "IcHNA9yisw4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Libraries Installation"
      ],
      "metadata": {
        "id": "ZiKWww5gxpyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai sentence-transformers numpy scikit-learn rich torch"
      ],
      "metadata": {
        "id": "aakvukXexuiX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Import libraries\n"
      ],
      "metadata": {
        "id": "h91PTyM7xyCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, torch, numpy as np\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rich.console import Console\n",
        "\n",
        "console = Console()"
      ],
      "metadata": {
        "id": "igyj1RGEx4jD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Setup"
      ],
      "metadata": {
        "id": "BufXuGdWx8TA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Device Setup ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "console.print(f\"[bold blue]Device detected:[/bold blue] {device}\")\n",
        "\n",
        "# üîë Gemini API Key (runs on Google infrastructure)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBORKiBmZFpNrP8W5wq8ZwemA6dpDVvdVo\"  # Replace with your actual key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Initialize Gemini 2.5 Flash (cloud execution)\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "console.print(\"[green]Gemini 2.5 Flash initialized successfully (runs on cloud).[/green]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "t6nsbuR8x9xg",
        "outputId": "241d5aa3-d250-4301-f697-46a3414a8eae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mDevice detected:\u001b[0m cuda\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Device detected:</span> cuda\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32mGemini \u001b[0m\u001b[1;32m2.5\u001b[0m\u001b[32m Flash initialized successfully \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mruns on cloud\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Gemini </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.5</span><span style=\"color: #008000; text-decoration-color: #008000\"> Flash initialized successfully </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">runs on cloud</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Create Knowledge base / Synthetic data"
      ],
      "metadata": {
        "id": "ZL_GzxP7yFdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = [\n",
        "    \"RAG (Retrieval-Augmented Generation) is an AI framework that combines an LLM with an external knowledge base to ground the model in current, accurate facts.\",\n",
        "    \"Agentic AI refers to a system where an LLM is given the ability to reason, plan, and choose which tool or step to execute next (like search, code execution, or final answer).\",\n",
        "    \"Agentic RAG specifically means the agent decides when and what to retrieve from the knowledge base, often using a 'Thought' or 'Decision' step.\",\n",
        "    \"The MiniLM model, 'all-MiniLM-L6-v2', is a highly efficient sentence-transformer, known for being fast with a small size (384 dimensions) while offering good performance.\",\n",
        "    \"Gemini 2.5 Flash is Google's most balanced and fast model, suitable for both the decision-making and final generation steps in this agentic workflow.\",\n",
        "    \"A typical RAG pipeline is a fixed sequence (Query -> Retrieve -> Generate), whereas Agentic RAG is a dynamic loop (Query -> Decide -> Act -> Generate).\",\n",
        "    \"Vector databases like FAISS or ChromaDB are often used to store the embeddings generated by models like MiniLM for efficient similarity search.\",\n",
        "]"
      ],
      "metadata": {
        "id": "IZBv6J-byKKH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Embedding model"
      ],
      "metadata": {
        "id": "XyJVvfTWyQbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(\"[bold blue]Embedding Knowledge Base with MiniLM on GPU (if available)...[/bold blue]\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        "\n",
        "# Encode the knowledge base (list of text chunks) into dense vector embeddings\n",
        "# using the MiniLM model. Each text becomes a numerical representation that captures its meaning.\n",
        "corpus_embeddings = embedder.encode(knowledge_base, convert_to_tensor=True, device=device)\n",
        "console.print(f\"[green]Knowledge base embedded successfully on {device.upper()}.[/green]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "E4K3HI4myTNG",
        "outputId": "c19f4194-5d0a-4509-e868-856f36e15bee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34mEmbedding Knowledge Base with MiniLM on GPU \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mif available\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Embedding Knowledge Base with MiniLM on GPU (if available)...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32mKnowledge base embedded successfully on CUDA.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Knowledge base embedded successfully on CUDA.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Helper Functions"
      ],
      "metadata": {
        "id": "p26Nqd8gyYBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query, top_k=2):\n",
        "    console.print(\"[dim]Action: Searching Knowledge Base...[/dim]\")\n",
        "    #    This converts the query (a sentence) into a 384-dimensional tensor,\n",
        "    #    so we can compare it numerically with all knowledge base embeddings.\n",
        "    query_emb = embedder.encode([query], convert_to_tensor=True, device=device)\n",
        "\n",
        "    #    Cosine similarity measures how \"aligned\" two vectors are in meaning.\n",
        "    #    Higher similarity ‚Üí closer semantic relationship.\n",
        "    sims = torch.nn.functional.cosine_similarity(query_emb, corpus_embeddings)\n",
        "    #    torch.topk() efficiently retrieves the highest similarity scores.\n",
        "    #    We move results to CPU and convert to NumPy for indexing and display.\n",
        "    top_idx = torch.topk(sims, top_k).indices.cpu().numpy()\n",
        "    #   Using the indices from above, we pull the corresponding text from the knowledge base.\n",
        "    docs = [knowledge_base[i] for i in top_idx]\n",
        "    console.print(f\"[dim]Top similarity score: {sims[top_idx[0]].item():.4f}[/dim]\")\n",
        "    return \"\\n\".join(docs)\n",
        "\n",
        "\n",
        "# This function sends a text prompt to the Gemini model, safely collects its generated response,\n",
        "# and returns the output text ‚Äî with built-in error handling in case the model returns nothing or fails.\n",
        "def gemini_generate(prompt, max_output_tokens=300, temperature=0.5):\n",
        "    \"\"\"Safe Gemini text generation with fallback handling.\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config={\"max_output_tokens\": max_output_tokens, \"temperature\": temperature},\n",
        "        )\n",
        "        if not hasattr(response, \"candidates\") or not response.candidates:\n",
        "            return \"[Gemini returned no candidates.]\"\n",
        "        parts = []\n",
        "        for c in response.candidates:\n",
        "            if hasattr(c, \"content\") and hasattr(c.content, \"parts\"):\n",
        "                for p in c.content.parts:\n",
        "                    if hasattr(p, \"text\"):\n",
        "                        parts.append(p.text)\n",
        "        return \" \".join(parts).strip() if parts else \"[Gemini produced no text.]\"\n",
        "    except Exception as e:\n",
        "        return f\"[Error: {e}]\"\n",
        "\n",
        "\n",
        "# This function asks Gemini to decide whether the agent should SEARCH for more context\n",
        "# or ANSWER directly, based on how complete or relevant the current context is ‚Äî\n",
        "# then returns both the final decision and the model‚Äôs raw reasoning text.\n",
        "def get_decision(query, context):\n",
        "    \"\"\"Decide SEARCH or ANSWER using adaptive reasoning.\"\"\"\n",
        "    decision_prompt = f\"\"\"\n",
        "You are an AI agent in an Agentic RAG system.\n",
        "\n",
        "You can do one of two actions:\n",
        "- SEARCH ‚Üí if you need more information.\n",
        "- ANSWER ‚Üí if you already have enough context to give the final answer.\n",
        "\n",
        "Question: {query}\n",
        "Current context: {context}\n",
        "\n",
        "If the context already seems to contain relevant facts (e.g. mentions of Agentic RAG or RAG),\n",
        "then you should ANSWER instead of SEARCH.\n",
        "\n",
        "Respond in one sentence that includes the word SEARCH or ANSWER.\n",
        "\"\"\"\n",
        "    decision_raw = gemini_generate(decision_prompt, max_output_tokens=100, temperature=0.6)\n",
        "    if \"ANSWER\" in decision_raw.upper():\n",
        "        return \"ANSWER\", decision_raw\n",
        "    elif \"SEARCH\" in decision_raw.upper():\n",
        "        return \"SEARCH\", decision_raw\n",
        "    else:\n",
        "        if \"RAG\" in context.upper():\n",
        "            return \"ANSWER\", decision_raw\n",
        "        return \"SEARCH\", decision_raw"
      ],
      "metadata": {
        "id": "lTA79dylybhO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Agentic Rag"
      ],
      "metadata": {
        "id": "DoQ5lyJyyn9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function runs the full Agentic RAG reasoning loop ‚Äî repeatedly asking Gemini to decide\n",
        "# whether to SEARCH for more context or ANSWER using what‚Äôs already known ‚Äî retrieving information\n",
        "# or generating the final answer accordingly, and printing each reasoning step for clarity.\n",
        "\n",
        "def run_agentic_rag(query, max_steps=3):\n",
        "    console.print(\"\\n[bold magenta]*** AGENTIC RAG PROCESS STARTING ***[/bold magenta]\")\n",
        "    console.print(f\"[bold cyan]User Query:[/bold cyan] {query}\")\n",
        "    context = \"No information retrieved yet.\"\n",
        "\n",
        "    for step in range(1, max_steps + 1):\n",
        "        console.print(f\"\\n[bold yellow]--- STEP {step} ---[/bold yellow]\")\n",
        "        decision, decision_raw = get_decision(query, context)\n",
        "        console.print(f\"[bold green]Agent Decision:[/bold green] {decision}\")\n",
        "\n",
        "        if decision == \"SEARCH\":\n",
        "            context = retrieve_context(query)\n",
        "            console.print(\"[bold]Retrieved Chunks:[/bold]\")\n",
        "            console.print(f\"RETRIEVED CONTEXT:\\n{context}\")\n",
        "\n",
        "        elif decision == \"ANSWER\" or step == max_steps:\n",
        "            answer_prompt = f\"\"\"\n",
        "You are completing the final step of an Agentic RAG process.\n",
        "\n",
        "Use the retrieved context to give a clear, factual final answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer in 2‚Äì3 sentences.\n",
        "\"\"\"\n",
        "            answer = gemini_generate(answer_prompt, max_output_tokens=300, temperature=0.4)\n",
        "            console.print(f\"[bold magenta]Final Answer:[/bold magenta] {answer}\")\n",
        "            break\n",
        "\n",
        "    console.print(\"[bold magenta]*** AGENTIC RAG PROCESS FINISHED ***[/bold magenta]\")\n"
      ],
      "metadata": {
        "id": "DAtWuKupyp_-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_agentic_rag(\"What makes Agentic RAG different from the normal, standard RAG process?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "bVwyGIC80tQp",
        "outputId": "195360ec-6529-4c9f-ccc9-175a144cd24b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35m*** AGENTIC RAG PROCESS STARTING ***\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">*** AGENTIC RAG PROCESS STARTING ***</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mUser Query:\u001b[0m What makes Agentic RAG different from the normal, standard RAG process?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">User Query:</span> What makes Agentic RAG different from the normal, standard RAG process?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m--- STEP \u001b[0m\u001b[1;33m1\u001b[0m\u001b[1;33m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">--- STEP </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent Decision:\u001b[0m SEARCH\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent Decision:</span> SEARCH\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mAction: Searching Knowledge Base\u001b[0m\u001b[2;33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Action: Searching Knowledge Base</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mTop similarity score: \u001b[0m\u001b[1;2;36m0.7277\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Top similarity score: </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">0.7277</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mRetrieved Chunks:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Retrieved Chunks:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RETRIEVED CONTEXT:\n",
              "A typical RAG pipeline is a fixed sequence \u001b[1m(\u001b[0mQuery -> Retrieve -> Generate\u001b[1m)\u001b[0m, whereas Agentic RAG is a dynamic loop \n",
              "\u001b[1m(\u001b[0mQuery -> Decide -> Act -> Generate\u001b[1m)\u001b[0m.\n",
              "Agentic RAG specifically means the agent decides when and what to retrieve from the knowledge base, often using a \n",
              "\u001b[32m'Thought'\u001b[0m or \u001b[32m'Decision'\u001b[0m step.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RETRIEVED CONTEXT:\n",
              "A typical RAG pipeline is a fixed sequence <span style=\"font-weight: bold\">(</span>Query -&gt; Retrieve -&gt; Generate<span style=\"font-weight: bold\">)</span>, whereas Agentic RAG is a dynamic loop \n",
              "<span style=\"font-weight: bold\">(</span>Query -&gt; Decide -&gt; Act -&gt; Generate<span style=\"font-weight: bold\">)</span>.\n",
              "Agentic RAG specifically means the agent decides when and what to retrieve from the knowledge base, often using a \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'Thought'</span> or <span style=\"color: #008000; text-decoration-color: #008000\">'Decision'</span> step.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m--- STEP \u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">--- STEP </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent Decision:\u001b[0m ANSWER\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent Decision:</span> ANSWER\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mFinal Answer:\u001b[0m Agentic RAG differs from standard RAG in its dynamic, rather than fixed, process. While standard RAG \n",
              "follows a set sequence of Query, Retrieve, then Generate, Agentic RAG operates as a dynamic loop \u001b[1m(\u001b[0mQuery -> Decide \n",
              "-> Act -> Generate\u001b[1m)\u001b[0m. This means the agent actively decides when and what to retrieve from the knowledge base, often\n",
              "through a \u001b[32m'Thought'\u001b[0m or \u001b[32m'Decision'\u001b[0m step.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Answer:</span> Agentic RAG differs from standard RAG in its dynamic, rather than fixed, process. While standard RAG \n",
              "follows a set sequence of Query, Retrieve, then Generate, Agentic RAG operates as a dynamic loop <span style=\"font-weight: bold\">(</span>Query -&gt; Decide \n",
              "-&gt; Act -&gt; Generate<span style=\"font-weight: bold\">)</span>. This means the agent actively decides when and what to retrieve from the knowledge base, often\n",
              "through a <span style=\"color: #008000; text-decoration-color: #008000\">'Thought'</span> or <span style=\"color: #008000; text-decoration-color: #008000\">'Decision'</span> step.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35m*** AGENTIC RAG PROCESS FINISHED ***\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">*** AGENTIC RAG PROCESS FINISHED ***</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_agentic_rag(\"What is MiniLM known for?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "LZ-ngmSu6H_L",
        "outputId": "8937c3ed-bd9b-4088-b801-f90e140f84af"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35m*** AGENTIC RAG PROCESS STARTING ***\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">*** AGENTIC RAG PROCESS STARTING ***</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mUser Query:\u001b[0m What is MiniLM known for?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">User Query:</span> What is MiniLM known for?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m--- STEP \u001b[0m\u001b[1;33m1\u001b[0m\u001b[1;33m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">--- STEP </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent Decision:\u001b[0m SEARCH\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent Decision:</span> SEARCH\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mAction: Searching Knowledge Base\u001b[0m\u001b[2;33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Action: Searching Knowledge Base</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mTop similarity score: \u001b[0m\u001b[1;2;36m0.5420\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Top similarity score: </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">0.5420</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mRetrieved Chunks:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Retrieved Chunks:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RETRIEVED CONTEXT:\n",
              "The MiniLM model, \u001b[32m'all-MiniLM-L6-v2'\u001b[0m, is a highly efficient sentence-transformer, known for being fast with a small\n",
              "size \u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m dimensions\u001b[1m)\u001b[0m while offering good performance.\n",
              "Vector databases like FAISS or ChromaDB are often used to store the embeddings generated by models like MiniLM for \n",
              "efficient similarity search.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RETRIEVED CONTEXT:\n",
              "The MiniLM model, <span style=\"color: #008000; text-decoration-color: #008000\">'all-MiniLM-L6-v2'</span>, is a highly efficient sentence-transformer, known for being fast with a small\n",
              "size <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span> dimensions<span style=\"font-weight: bold\">)</span> while offering good performance.\n",
              "Vector databases like FAISS or ChromaDB are often used to store the embeddings generated by models like MiniLM for \n",
              "efficient similarity search.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m--- STEP \u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">--- STEP </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent Decision:\u001b[0m ANSWER\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent Decision:</span> ANSWER\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mFinal Answer:\u001b[0m MiniLM is known as a highly efficient sentence-transformer model. It is particularly recognized for \n",
              "being fast and having a small size \u001b[1m(\u001b[0m\u001b[1;36m384\u001b[0m dimensions\u001b[1m)\u001b[0m while still offering good performance.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Answer:</span> MiniLM is known as a highly efficient sentence-transformer model. It is particularly recognized for \n",
              "being fast and having a small size <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span> dimensions<span style=\"font-weight: bold\">)</span> while still offering good performance.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35m*** AGENTIC RAG PROCESS FINISHED ***\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">*** AGENTIC RAG PROCESS FINISHED ***</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_agentic_rag(\"What is the main difference between Agentic RAG and a typical RAG pipeline?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "oyjnOu506MV3",
        "outputId": "26ec8b00-fc4e-4f46-caa9-f2057e627159"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35m*** AGENTIC RAG PROCESS STARTING ***\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">*** AGENTIC RAG PROCESS STARTING ***</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mUser Query:\u001b[0m What is the main difference between Agentic RAG and a typical RAG pipeline?\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">User Query:</span> What is the main difference between Agentic RAG and a typical RAG pipeline?\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m--- STEP \u001b[0m\u001b[1;33m1\u001b[0m\u001b[1;33m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">--- STEP </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent Decision:\u001b[0m SEARCH\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent Decision:</span> SEARCH\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mAction: Searching Knowledge Base\u001b[0m\u001b[2;33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Action: Searching Knowledge Base</span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mTop similarity score: \u001b[0m\u001b[1;2;36m0.8403\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Top similarity score: </span><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf; font-weight: bold\">0.8403</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mRetrieved Chunks:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Retrieved Chunks:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RETRIEVED CONTEXT:\n",
              "A typical RAG pipeline is a fixed sequence \u001b[1m(\u001b[0mQuery -> Retrieve -> Generate\u001b[1m)\u001b[0m, whereas Agentic RAG is a dynamic loop \n",
              "\u001b[1m(\u001b[0mQuery -> Decide -> Act -> Generate\u001b[1m)\u001b[0m.\n",
              "Agentic RAG specifically means the agent decides when and what to retrieve from the knowledge base, often using a \n",
              "\u001b[32m'Thought'\u001b[0m or \u001b[32m'Decision'\u001b[0m step.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RETRIEVED CONTEXT:\n",
              "A typical RAG pipeline is a fixed sequence <span style=\"font-weight: bold\">(</span>Query -&gt; Retrieve -&gt; Generate<span style=\"font-weight: bold\">)</span>, whereas Agentic RAG is a dynamic loop \n",
              "<span style=\"font-weight: bold\">(</span>Query -&gt; Decide -&gt; Act -&gt; Generate<span style=\"font-weight: bold\">)</span>.\n",
              "Agentic RAG specifically means the agent decides when and what to retrieve from the knowledge base, often using a \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'Thought'</span> or <span style=\"color: #008000; text-decoration-color: #008000\">'Decision'</span> step.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m--- STEP \u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m ---\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">--- STEP </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> ---</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mAgent Decision:\u001b[0m ANSWER\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Agent Decision:</span> ANSWER\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mFinal Answer:\u001b[0m The main difference is that a typical RAG pipeline follows a fixed sequence \u001b[1m(\u001b[0mQuery -> Retrieve -> \n",
              "Generate\u001b[1m)\u001b[0m, while Agentic RAG operates as a dynamic loop \u001b[1m(\u001b[0mQuery -> Decide -> Act -> Generate\u001b[1m)\u001b[0m. Agentic RAG \n",
              "specifically incorporates an agent that decides when and what to retrieve from the knowledge base, often through a \n",
              "\u001b[32m'Thought'\u001b[0m or \u001b[32m'Decision'\u001b[0m step, making it more adaptive.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Final Answer:</span> The main difference is that a typical RAG pipeline follows a fixed sequence <span style=\"font-weight: bold\">(</span>Query -&gt; Retrieve -&gt; \n",
              "Generate<span style=\"font-weight: bold\">)</span>, while Agentic RAG operates as a dynamic loop <span style=\"font-weight: bold\">(</span>Query -&gt; Decide -&gt; Act -&gt; Generate<span style=\"font-weight: bold\">)</span>. Agentic RAG \n",
              "specifically incorporates an agent that decides when and what to retrieve from the knowledge base, often through a \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'Thought'</span> or <span style=\"color: #008000; text-decoration-color: #008000\">'Decision'</span> step, making it more adaptive.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35m*** AGENTIC RAG PROCESS FINISHED ***\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">*** AGENTIC RAG PROCESS FINISHED ***</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî¨ 10. Student Exercises\n",
        "\n",
        "Modify corpus: add 5‚Äì10 of their own knowledge sentences.\n",
        "\n",
        "Change model: try microsoft/phi-2 or Qwen/Qwen2.5-3B-Instruct.\n",
        "\n",
        "Add reflection: ask model to judge its own answer:\n",
        "‚ÄúWas your answer sufficient? If not, what else do you need?‚Äù\n",
        "\n",
        "Visualization: draw the reasoning loop (plan ‚Üí retrieve ‚Üí generate ‚Üí reflect).\n",
        "\n",
        "Optional (advanced): connect to Wikipedia via wikipedia API for live retrieval."
      ],
      "metadata": {
        "id": "EM0ggNdA2C1D"
      }
    }
  ]
}